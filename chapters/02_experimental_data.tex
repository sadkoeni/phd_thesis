\section{Experimental data and experimental method}\label{sec:ExperimentAndMethod}

\subsection{ALICE}
This section aims to highlight the capabilities of the ALICE detector, in particular in the context of identifying antinuclei and measuring their inelastic cross sections. Using the ALICE detector for this analysis is interesting on many levels, including in terms of experimental methods, since the ALICE detector was not designed with such a measurement in mind. Rather, the excellent tracking and particle identification capabilities of the detector are what enable the measurement in first place. We shall therefore discuss the full chain of experimental methods, starting from the particle identification in each of the detectors, to the ALICE data structure and how they are used to obtain the antiparticle-to-particle ratios. 

\subsubsection{Overview}
A Large Ion Collider Experiment (ALICE) is one of the four major experiments at the large hadron collider (LHC) in Geneva, Switzerland. It is the only dedicated heavy-ion experiment at the LHC, with the main physics motivation being the study of quark-gluon-plasma (QGP). The experiment has 19 subdetector systems \cite{}, of which the most important for this analysis are the Time-Projection-Chamber (TPC), the Inner Tracking System (ITS) and the Time-of-Flight detector (TOF). In particular the TPC sets ALICE apart from the other major LHC experiments, by enabling very precise tracking of particles, good particle identification via momentum and the specific energy loss measurements, and its sensitivity to particles down to low momentum (down to $p_T$<0.2 GeV/$c$). The momentum measurement is enabled by a solenoid magnetic field, which is usually operated at 0.5T\footnote{There are also dedicated low B field runs, where the field is set to only 0.2T.}. The central detector systems are constructed in a cylindrical shape, providing full azimuthal coverage. The coverage in the forward and backward direction can conveniently be described using the measure of pseudorapidity ($\eta$), which is defined as $\eta (\theta) = -\mathrm{ln}[tan(\theta/2)]$, where $\theta$ is the angle which the emitted particle has to the beam axis. This measure is anti-symmetric around $\theta = \pi/2$, i.e. the angle normal to the beam axis, and is 0 at this angle. Small absolute values of $\eta \lesssim 1$ are referred to as midrapidity, while particles emitted in the beam direction ($|\eta|\gtsim 1$) are called for forward $\eta >0$ and backward $\eta < 0$. The central detectors of ALICE cover the midrapidity range of $|\eta|\lesssim 1$. A schematic representation of ALICE with all its subdetector systems can be found in figure \ref{fig:ALICE}. It is important to note that the detector discussed in this section is the ALICE detector as it existed during the Run 2 data taking period (2015-2018). The main limitation of this version of the ALICE detector were its low interaction and data readout rates, the latter of which was limited to about 1kHz. This is due to the 1$\mu s$ timespan of reading out a single event, which is dead-time for the detector. The upgrade for LHC Run3 which has started in 2022 will instead be able to read out data at rates up to 50 kHz, and provide a significant boost to the statistics which the ALICE detector can provide.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ALICE_figure.jpeg}
    \caption{Schematic representation of the ALICE detector and its subdetector systems, during the Run 2 data taking period (2015-2018). Figure taken from \cite{}.}
    \label{fig:ALICE}
\end{figure}

\subsubsection{The ALICE Trigger System}
The ALICE detector cannot read out all events which occur, both due to the detector dead-time when reading out an event and due to the data rates which would be involved. Instead, interesting events are selected by predefined criteria, and then triggered upon. This trigger system then initiates the entire read-out sequence of the detector. The most basic trigger is the so called minimum bias (MB) trigger, which should trigger in the presence of any collisions and not introduce any bias based on the occurring physics. This is an important data sample to check against, however, it is interesting to bias the selected events in favor of "more interesting" physics. Such triggers look for less common conditions more favorable to rare physics events, such as e.g. the presence of more charged particles. These triggers exploit the fact that data acquisition is limited by the ALICE read-out rate -- not by the occurrence rate of rare events -- in order to collect data for rare events at the same 1kHz rate at which MB data can be collected. \\

For pp collisions, the trigger-criterion used in the analyses presented in this thesis is the multiplicity in the V0 detectors (V0A and V0C), which is used as a proxy for the charged particle multiplicity at midrapidity. The V0 detectors are plastic scintillator arrays in the forward and backwards regions, covering a pseudorapidity range of $x2.8<\eta<5.1$ and $-3.7<\eta<-1.7$, respectively. They are located 3.4 m and 0.9 m from the interaction point. The high multiplicity trigger (kHighMult) is configured so that the highest 0.17\% of multiplicity events are selected by the V0 detectors, while also requiring a minimum of 1 charged particle at midrapidity. This proxy works well for high multiplicities also at midrapidity, as this selection in an average of 30-40 charged particles at mid-rapidity. 

\subsubsection{Inner Tracking System (ITS)}
The Inner Tracking System (ITS) is the innermost detector in ALICE, staring at a radius of just 3.9 cm from the interaction point and reaching a radius of 43 cm. It consists of 3 lightweight silicon bases sub-systems, called the Silicon Pixel Detector (SPD), the Silicon Strip Detector (SSD) and the Silicon Drift Detector (SDD). The ITS covers a pseudorapidity range of $|\eta|<0.9$\footnote{The SPD can detect particles with a wider range, up to $|\eta|<1.95$.}. A schematic of the ITS is shown in figure \ref{fig:ITS_schematic}. Since the ITS is the closest detector to the interaction point, it plays a vital role in determining the position of the primary vertex. Indeed, its ability to accurately reconstruct particle trajectories enables the reconstruction of the primary vertex to a precision of 100 $\mu$m, and constrain the DCA distributions of emitted particles. This is particularly important when analysing nuclei at low momenta, since for deuterons, $^3\mathrm{He}$ and $^3\mathrm{H}$ the contribution from secondaries from material spallation is the dominant contamination in the nuclei signal. The requirement of a cluster in the first ITS layer (SPD), removes any tracks from particles which get removed from material further out, unless by chance there is a matchable cluster on their trajectory.  The ITS also allows the rejection of pile-up events\footnote{Pile-up is what happens when tracks from a different physical collision are incorrectly matched to the same event.}. The particle identification capabilities of the ITS become less reliable for very large specific energy loss ($dE/dx$), due to saturation effects, which makes the ITS less useful for PID of doubly charged particles such as $^3\mathrm{He}$.

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/ITS_figure.jpeg}
    \caption{A schematic of the ALICE Inner Tracking System, taken from \cite{}. The three layer groups (SPD, SSD, SDD) are marked. }
    \label{fig:ITS_schematic}
\end{figure}

\subsubsection{Time Projection Chamber}
The Time Projection Chamber (TPC) is the main tracking detector of the ALICE experiment. It follows the ITS in the central barrel, at radii from 85 cm to 247 cm from the interaction point, covering a pseudorapidity range of $|\eta|<0.9$. The schematic layout of the TPC is shown in figure \ref{fig:TPC_schemtic}. It consists of a gas filled field cage, which can measure the ionisation cause by charged particles travelling through the gas. Due to the electric field of X V/m, the electrons from ionisation drift towards the read out cathodes of the field cage. The amplitude of the signal gives a measure of the specific energy loss of the particles (dE/dx), while the position of the clusters at the readout cathode gives a 2-dimensional (x and y) position of the tracks of the particles. Finally, by measuring the time of arrival of the electrons relative to the timing of the initial collision, the z position can be calculated. This method is shown on the right of figure \ref{fig:TPC_schemtic}. Measuring the position and therefore the curvature of the track allows the determination of the momentum of the particles. 

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/tpc_cage.png}
    \includegraphics[width=0.49\textwidth]{figures/tpc_process.png}
    \caption{Left: Schematic of the field cage of the TPC detector \cite{}. Right: Schematic of the reconstruction mechanism for tracks in the TPC \cite{ALICE_TPC_trigger}.}
    \label{fig:TPC_schemtic}
\end{figure}

Due to the combination of a momentum and a specific energy loss measurement, the TPC has amazing particle identification abilities. The energy loss of relativistic particles\footnote{At very low energies below $\lesssim 0.5 $MeV, the Bethe-Bloch formula does not apply. } is given by the Bethe-Bloch formula, which is reproduced in equation \ref{eq:Bethe-Bloch}:

\begin{equation}\label{eq:Bethe-Bloch}
    -\frac{dE}{dx} = \frac{4\pi n z^2}{m_e c^2 \beta^2} \left( \frac{e^2}{4\pi \epsilon_0}\right)^2 \left[ \mathrm{ln}\left(\frac{2m_ec^2\beta^2}{I(1-\beta^2)}\right)-\beta^2 \right]
\end{equation}
, where $n$ is the electron density of the material, $I$ is the mean excitation energy of the material, and the other symbols have their usual meaning. Since equation \ref{eq:Bethe-Bloch} is a function of only $\beta$ and $z$ for a given material, a measurement of both the momentum $p=\frac{\beta}{\sqrt{1-\beta^2}}m$ and the energy loss will differentiate particles of different masses. This differentiating power of the TPC is shown in figure \ref{fig:PID_TPC}. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/Ali_TPC_performance_pp.pdf}
    \caption{Specific energy loss in the TPC as a function of the rigidity $p/z$. Due to their different masses, different particles can be differentiated according to equation \ref{eq:Bethe-Bloch}. This shows the differentiating power of the TPC for low momentum particles.}
    \label{fig:PID_TPC}
\end{figure}

\subsubsection{Time-of-flight detector (TOF)}

As can be seen from figure \ref{fig:PID_TPC}, the differentiating power of the TPC decreases drastically at higher momenta, as the energy loss of particles tends towards the value for a minimum ionising particle (MIP). In order to differentiate between particles of different masses at higher momenta, an additional detector is required. The detector use for this purpose in ALICE is the Time-of-flight (TOF) detector. The TOF is a silicon pad detector, which measured the time difference between the initial collision and the formation of a cluster in one of its readout pads. It is arranged in a cylindrical symmetric structure between 370 cm and 399 cm from the interaction point and has the same coverage in pseudorapidity as the TPC ($|\eta| < 0.9$). The TOF is mounted in a steel structure called the space frame\cite{ALICE_TOF_TDR, ALICE_TDR}. A schematic representation of the TOF detector is shown in figure \ref{fig:TOF_schematic}. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/TOF.png}
    \caption{The TOF detector of the ALICE experiment, taken from \cite{}.}
    \label{fig:TOF_schematic}
\end{figure}

The time resolution of the TOF readout pads is $\approx 50$ ps\footnote{In order to get an idea }.
Thus, a measure of the particle velocity, called the TOF beta, can be measured as $\beta = L/t$, where $L$ is the length of the track on its curved trajectory through the TPC, and $t$ is the measured time-of-flight. From the relation $p = \gamma \beta m$, equation \ref{eq:TOFm2} can be derived, which relates the measured $\beta_{\mathrm{TOF}}$ to the tracks mass. The factor $1/Z^2$ cannot be neglected here since the detector cannot know the particles mass a piori. Thus, when analysing multicharged particles such as \ahe\ , the charge observable from the TOF is $m_{\mathrm{TOF}}/Z^2$. 

\begin{equation}\label{eq:TOFm2}
    m^2/Z^2 = \frac{p^2}{c^2 Z^2}\left(\beta_{\mathrm{TOF}}^2\gamma^2 \right) = \frac{p^2}{c^2 Z^2} \left( \frac{c^2t^2}{L^2} -1\right)
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/2016-Sep-08-beta.pdf}
    \caption{Measurement of the track velocity $\beta_{\mathrm{TOF}}$ from the TOF detector in pp collisions at $\sqrt{s}=13$ TeV. $\beta_{\mathrm{TOF}}$ is classically measured as length of the track divided by the time-of-flight.}
    \label{fig:TOF_performance}
\end{figure}

The performance of the TOF detector is shown in figure \ref{fig:TOF_performance}. A clear separation between particles can be seen up to much higher momenta than in the TPC. This is particularly true for higher mass particles. The deuteron line can be seen below the marked proton line, and is well differentiable well beyond the merging of the proton and pion lines. 



\subsubsection{Basics of ALICE data structure}
ALICE data is split by runs, then by events, and within the event by tracks, as is shown in figure \ref{fig:ALICE_data_schematic}. Runs are periods of time during which collisions with the same conditions occurred, which means that the data taking is started and kept up until there is some problem which requires the run to be ended. This means that runs are of arbitrary length. They are denoted by LHC + the last two digits of the year in which the data was taken + a letter of the alphabeth, sequentially used, e.g.: LHC17a is the first data run taken during 2017. Once the raw data is taken, the Data Preparation Group (DPG) is responsible for doing a reconstruction pass over the data, which means to build the tracks from the individual detector hits, correcting for any calibration or distortion effects. The data structure  one is left with is a list of events, each of which contain a list of tracks. This is what is subsequently used by analysers. 

\begin{figure}
	\includegraphics[width=\textwidth]{figures/data_structure.png}
	\centering
	\caption{Schematic of the data structures within ALICE. The data is split by run periods, then by event, and within the event by tracks. }
		\label{fig:ALICE_data_schematic}
\end{figure}
\subsubsection{Collision system and event cuts}
The data provided in ALICE by necessity includes a large range of particles and impurities, since for each analysis what qualifies as impurities changes. Therefore, cuts are applied at the analysis level, to provide a much cleaner environment for the actual analysis. Within the analysis, these cuts happen on both an event and a track level, leaving a subset of tracks which can be analyzed. The goals of these cuts are: i) to cut bad quality tracks, where the PID is not certain, ii) to cut tracks of uninteresting particles for the specific analysis, e.g. weak decays in the analyses in this thesis and iii) to reduce the background, such as from secondary particles. These cuts also vary between collision systems, which is necessitated by their different properties. To exemplify, lets compare high multiplicity pp and Pb--Pb collisions. In HM pp collisions the mean multiplicity is 34, while in central lead lead collisions it is about 1000. This means that the mean occupancy of the detectors is much greater in Pb--Pb collisions, which in turn means that the tracking algorythm has a higher chance to assign a wrong cluster to a track. In order to reduce this effect, the matching window for the TOF detector is reduced in Pb--Pb collisions, from 10 cm to 3 cm. For the analysis method explained in section \ref{sec:TOFTPC}, this introduces an uncertainty, as some tracks could be elastically scattered in the TRD or space frame, causing them to miss the matching window without having interacted inelastically. This would mean that there is an elastic contribution to the inelastic cross section meassured using the TOF/TPC method. To evaluate and counteract this, a special reconstruction of the Pb--Pb data was done for the Pb--Pb results shown in this thesis, where the matching window was set to 10 cm instead of 3 cm. The effect of this change is explained in section \ref{sec:TOFTPC}. \\

The main selection criteria for the event was its multiplicty. The multiplicity measures the number of charged particles at mid-rapidity, and for this analysis the top 0-0.17\% of high multiplicity pp events were used. This data is selected by a trigger, called the high multiplicity trigger (using the kHighMult flag in the ALICE analysis framework), which triggers on the V0 multiplicity measurement. HOW ARE THESE RELATED? 

\subsubsection{Reconstruction of raw (anti)nuclei spectra}
In order to obtain the raw antinuclei spectra, the tracks first have to be identified as antinuclei. This particle identification (PID) occurs on the basis of two main detectors: the TPC and the TOF. Due to the distinct masses of antinuclei (they are heavier than most other long lived particles), they leave a distinct signal in each detector. In the TPC, antideuterons are clearly seperated by their energy loss up to a momentum of about 1.4 GeV. For antitriton, the seperation works until a momentum of 1.X GeV. \ahe\ is well sepatared from lighter particles in the TPC for all momenta, due to its double charge. Since the energy loss rises with $Z^2$, the energy loss of \ahe\ is characteristically much higher than those of singly charged particles. \\
In the TOF, the time of flight measurement combined with the track length and curvature gives a measurement of the particles mass, according to equation \ref{eq:TOFm2}. This allows a clean signal for antitriton and \ahe\. For antideuterons, there is still a significant contamination from the tail of the proton distribution at those masses, which requires a fit to the signal and the background to extract the antideuteron yield. Figures \ref{fig:TPC_fits_ahe} and \ref{fig:TOF_fits_ahe}, \ref{fig:TPC_fits_trit} and \ref{fig:TOF_fits_trit}, show the extraction procedure in the TPC and TOF, for \ahe\ and \atrit\, respectively. The particle and antiparticle distributions are fit with a gaussian function. For \ahe\ , a second gaussian is used to account for the background from (anti)triton. Their respective signals in the TOF detector are very clean, as is shown in figures \ref{fig:TOF_fits_ahe} and \ref{fig:TOF_fits_trit}, therefore, the TOF signal is used by applying a cut on the $m_{TOF}^2$. An example figure of the antideuteron extraction in the TOF is shown in figure \ref{fig:dbar_TOF_fit}(for more details see \cite{dbar_ann}). The combination of these measurement allows the extraction of the (anti)nuclei spectra, which are shown in \ref{fig:(anti)nucleiSpectra}. The combined signal extraction cuts are shown in table \ref{tab:PIDcuts}.


\begin{table}
%todo insert table of PID cuts from analysis notes.
\end{table}

%todo: insert all the PID figures from the he3bar and atrit analyses.
%todo: write a paragraph and insert figures on different fitting algorythms in AliPhysics, what they do and how this affects low statistics analyses -> i.e. a lot. Also talk about errors on the 0 bins here. 
\subsubsection{Raw \ratio\ \ and antitriton-to-triton ratio}
\subsubsection{Correction for secondaries from material spallation}
In order to obtain pure samples of nuclei, any secondary nuclei not created need to be subtracted from the obtained raw spectrum. Two sources of secondary particles exists: weak decays and material spallation. For \ahe\ and \atrit\, weak decays are negligible, since the amount of $^3H_\Lambda$ measured in pp collisions is xx the amount of \ahe\, as can be seen from figure \ref{}. The branching ratio of $^3H_\Lambda \rightarrow $ \ahe\ is expected to be 25\% \cite{PDG}. Thus, secondary nulcei from material spallation remain, which shall simply be referred to as secondaries hereinafter. Since these secondaries are created by essentially "knocking out" these nuclei from larger nuclei in the ALICE detector material and in the beampipe, no secondary antinuclei exist. In order to differentiate between secondaries and primaries, we make use of the fact that all primaries have a common origin (called the primary vertex), while the distribution of secondaries should not point to the primary vertex. The measure of how close a particle's track reaches to the primary vertex is known as the distance of closest approach (DCA), and within ALICE is resolved in both the $xy$ and the $z$ planes. So, from physical expectations we expect the primary DCA distribution to be peaked sharply at 0, while the distributions for secondaries should be mainly flat. Example distributions from Monte Carlo Simulations are shown in figure \ref{fig:}.


%todo include figure for hypertriton to he3 ratio
%todo include figure for exemplary DCA distributions 
Figure \ref{fig:} shows that while the distribution for primaries is indeed sharply peaked at 0, the distribution of secondaries is not flat, but also peaks around 0. This is an experimental effect due to the tracking algorithm, which prefers reconstructing tracks pointing towards the primary vertex. This is excacerbated by the possibility to assign a wrong ITS cluster to the track. Several cuts can be made on the tracks to minimize this effect, which are outlined in section \ref{sec:event and track selection}. The most important cut is on the number of clusters in the first ITS layer, which reduces the number of secondary tracks by XX\%, as is shown in figure \ref{fig:thatone->} %todo insert figure here showing the distributions before and after this cut.
\\
In order to extract the secondary fraction from the DCA distributions, template fits are used. These fits take the shape of input templates (in this case from primaries and from secondaries) and try to match their relative contribution in order to reproduce the shape in data. Two different fitting algorithms exists for this purpose within the ALICE analysis framework: the TFractionFitter and Roofit. The main difference between the two is that the TFractionFitter can change the shape of the templates within uncertainties in order to better reproduce the data. In the limit of infinite statistics, both of these methods should produce the same result. In the analyses shown in this thesis, the TFractionFitter was used as the default method, and Roofit was used to crosscheck these results. \\
The biggest challenge with secondary corrections is to get reliable templates for secondary nuclei from material. For a combinatorial background, a side band analysis can be done, since no deviating behaviour in the signal region is expected, but since we have already seen in figure \ref{} that secondary tracks are also peaked towards the primary vertex, the sideband analysis cannot help us account for this. It is however also impossible to extract a pure secondary distribution from data, since the peak region always necessarily includes the particles produced in the initial collision. Thus, we need to simulate the distribution with Monte Carlo simulations. This means that we rely on the assumption that all the involved processes are accurately reproduced in Monte Carlo, specifically, we rely on the fact that the angular distribution of the spallation processes are accurately reproduced\footnote{The absolute value of the cross section is not important for the accuracy of the templates, since the relative weight is later determined by the template fits. However, too low a cross section means that far more events have to be simulated in order to gain sufficient statistics to obtain the template.}. The advantage of this method, is that in full ALICE Monte Carlo simulations, the same tracking algorithm is used as in data reconstruction, which means that if the spallation processes are accurately simulated, the distribution will match the true distribution. Also, these simulations rely on the correct underlying event, i.e. for high multiplicity pp collisions, such collisions need to be accurately simulated. This is due to the fact that the spallation is triggered by particles produced in the primary collision. A final challenge to obtaining the template fits is the rarity of these spallation processes in MC simulations. \\

%write about rarity of these processes, back it up with the general purpose calculations, write for the need for the special reconstruction, and compare Geant3/Geant4 distributions


One could also ask the question of which primary particles are responsible for the largest amounts of nuclei secondaries from spallation. In order to investigate this question, a toy Monte Carlo simulation was used, where beams of primary particles were fired on layers of beryllium and beryllium + carbon, corresponding to the materials of the beampipe and the support structure of the first ITS layers. This configuration was chosen since if the spallation occurs later, the missing hit in the first ITS layer allows a large degree of rejection\footnote{This is somewhat less true in Pb-Pb collisions, since the multiplicities are so much higher and therefore a wrongly associated ITS cluster is far more likely.}. An exponential energy spectrum was used for the primary particles, tuned to the proton spectrum measured by ALICE \cite{}. Geant4 was employed for this simulation. The resulting yields of secondary deuterons and $^3\mathrm{He}$ are shown in figure \ref{fig:toyMCGeant4Spallation}. Interestingly, the primary antiparticles produce a larger portion of the secondary nuclei than their particle partners. Also, while antideuterons produce a larger amount of secondary nuclei than antiprotons (by roughly 2x), given that their relative abundance in pp collisions is 1000x less, their contribution is expected to be on the sub \% level. This leads to the conclusion that it is mainly (anti)protons and pions which are responsible for creating secondary antinuclei. Therefore, when using ALICE Monte Carlo simulations, it is not necessary to employ a coalescence afterburner with the underlying event in order to accurately simulate the secondary distributions. A caveat to this is that the simple toy Monte Carlo simulation only probed absolute yields, rather than the angular distribution, and as already noted above, the latter is the important factor. However, given that the contribution to the yields is on the sub \% level, any difference in the distribution is expected to be negligible. 

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=\textwidth]{figures/simpleMC_secondaries_G4.png}
    \caption{Results of a simple toy Monte Carlo simulation of a particle beam on materials mimicking the LHC beampipe in ALICE and the beampipe + ITS support structure. The resulting secondary deuterons and $^3\mathrm{He}$ are shown as a function of the primary particle fired, where the results are roughly scaled by the primary particles relative abundance.}
    \label{fig:toyMCGeant4Spallation}
\end{figure}
\subsubsection{Annihilations within the detector}
Annihilations within the detector material can occur at any point within the detector, but are of course more likely in denser materials. For the purpose of this discussion we shall differentiate between 3 different scenarios: i) annihilations before the middle of the TPC, since such tracks cannot be identified and will therefore not be reconstructed in our spectra. ii) annihilations between the middle of the TPC and the TOF, since these annihilations can be directly probed by the comparison of the yields in the TPC and TOF. And finally annihilations outside of the TOF detector, which for the purposes of this analysis is not seen at all, i.e. such annihilations are not measured. \\

Let us first consider the case where the annihilation occurs before the middle of the TPC. A track with less than half of the TPC clusters will be removed by the track cuts, therefore this track will not show up in our analysis, and will never even be identified as a \ahe\ candidate track. The situation is slightly different when considering tracks which annihilate between the TPC and the TOF. Those tracks can be identified in the TPC. For \ahe\ this identification can occur over the whole momentum range ($0.5<p/Z<4$ GeV/$c$ in HM pp collisions), while for antideuterons this identification only works up to $p<XX$ GeV/$c$ and for \atrit\ it only works up to $p<XX$ GeV/$c$. However, since the antinucleus does not reach the TOF, the TOF hit will either be missing, or at a wrong time (i.e. giving an incorrect TOF mass). This allows for two options in these analyses: in the case where the TPC is sufficient to clearly identify the antinucleus, and it is within the acceptance of the TOF, the difference between the TPC and TOF yields can be used in order to probe the antinuclei inelastic cross section without being reliant on the corresponding nuclei yields. The second option is to use the TOF information in order to increase the amount of material which the particles need to traverse before being considered in the analysis, which makes the ratio more sensitive to the inelastic cross section. This increase is rather drastic, since the material budget increases by a factor of $\approx 5$ when switching between a TPC only analysis and one which includes the TOF. This can be seen in figure \ref{fig:ALICE_detector_material_budget}, which shows the cumulative material budget in ALICE as a function of radius. \\

We are thus left with the two possible methods for measuring annihilations within our detector. The first is based on comparing the yields in the TPC and the TOF, in reagions where the antinucleus can be clearly identified in the TPC alone, and which include the acceptance of the TOF, and the second is based on quantifying the loss of particles as they move through the detector, which a priori works for any particles and momentum range which the detectors can probe\footnote{As we will have seen in section \ref{sec:secondary}, the unreliability of the secondary correction at low momentum limits the low momentum reach of this method.}.

\begin{figure}[bhtp]
		\centering
		\includegraphics[width=\textwidth]{figures/alice-material_0.png}
		\caption{Cumulative material budget of the ALICE detector, as a function of radius from the beampipe, taken from \cite{antideuteronXS}.}
		\label{fig:ALICE_detector_material_budget}
\end{figure}    

\subsection{Extracting the inelastic cross section from the antimatter-to-matter ratio}
The idea behind using the antimatter-to-matter ratio as the observable to measure the antinuclei inelastic cross section, is that antinuclei will annihilate in the detector material, and therefore disappear from our measurement. In order to quantify the inelastic cross section we thus need to know how many particles were originally produced, i.e. we need to normalise the antinuclei spectrum to the number of originally produced antinuclei. However, we cannot use theoretical predictions tuned to this data, since that would be a circular argument, i.e. we would get out the same inelastic cross section as we put in. Therefore, the matter nuclei are used as a proxy instead. This works very well for a few reasons. First, the matter inelastic cross section can be easily measured, and have been measured for deuterons \cite{}, $^3\mathrm{He}$\cite{} and tritons\cite{}. Second, other effects affecting acceptance or efficiency will largely cancel between the nuclei and antinuclei counterparts, since the two only differ in their charge sign. Third and perhaps most important, is the fact that at LHC energies, the baryochemical potential is very close to 0, and has been accurately measured for antiprotons. This means that we know to a very high degree of accuracy how many antinuclei are produced relative to the produced nuclei, and the other processes by which both might be lost within the detector are also well understood. Thus, the antimatter-to-matter ratio is sensitive to the antinuclei inelastic cross section, and other variables it is sensitive to are well understood and under control. This makes this ratio such a promising probe to measure the inelastic cross section.\\

Having established that the antimatter-to-matter ratio is sensitive to the inelastic cross section, it is still not trivial to extract the inelastic cross section from this observable. This difficulty is due to having to account for many processes. One example is the path which the particles take through the detector. In the magnetic field, (anti)nuclei travel on curved tracks, so the amount of matter they interact with will depend on their initial trajectory. This thus needs to be averaged over the $\eta$ distribution of the antinuclei. This is just one of many similar effects which make an analytical relationship between the antimatter-to-matter ratio and the antinuclei inelastic cross section difficult to achieve. Thankfully, there is a superior option with detailed Monte Carlo simulations. Detailed simulations of the ALICE detector using Geant4 account for such processes, and by changing the inelastic cross section in these simulations, we can probe its relationship to the antinuclei-to-nuclei ratio.
\subsubsection{Comparison of ratios with Monte Carlo simulations}\label{sec:MCSim}
In order to fairly compare the Monte Carlo simulations to the produced data, it is vital to account for the baryochemical potential\footnote{In other words: how much more antimatter particles we have for each matter particle. Given that we collide purely matter particles, there is a penalty for producing antimatter, even though at such high energies it is vanishingly small.} at such high energies. The relevant ratio of antiprotons to protons is shown in figure \ref{fig:BaryochemicalPotential}. Based on the same arguments as the formula for the coalescence parameter \ref{eq:CoalescenceParameter}, the effect on the ratio of antinuclei will be the same as to the antiproton-to-proton ratio taken to the exponent of the mass number of the antinucleus. 

\begin{figure}
    \centering
    \includegraphics{}
    \caption{Ratio of antiprotons to protons produced at mid-rapidity as a function of beam rapidity. At LHC energies the value approaches unity, demonstrating that at such high energies antimatter and matter are produced in almost equal amounts.}
    \label{fig:BaryochemicalPotential}
\end{figure}
\subsection{Evaluating the average ALICE material budget}


\subsubsection{Ratios as a function of the inelastic cross section}
In order to extract the inelastic cross section measurement from the antiparticle-to-particle ratio, we have to compare the measured ratio in each bin to values from MC simulation with varied values of the inelastic cross section. Thus, it is useful to plot the measured ratios in simulation as a function of the inelastic cross section. These plots are shown for the $^3\overline{\mathrm{He}}/{^3\mathrm{He}}$ and $^3\overline{\mathrm{H}}/{^3\mathrm{H}}$ ratios in figures \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3He} and \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3H}, respectively. These plots also show fit lines to the Monte Carlo points, which were with with an exponential according to the Lambert-Beer absorption law \cite{}, which is reproduced in equation \ref{eq:LambertBeer}

\begin{equation}\label{eq:LambertBeer}
    N/N_0 = \mathrm{exp}\left( -x\times \sigma \times k \right)
\end{equation}
, where x is the distance travelled through a medium, $\sigma$ is the absorption cross section and k is a constant dependent on the parameters of the medium. This can be understood as a general absorption law. Since the only difference between the different Monte Carlo simulations is the implemented inelastic cross section, $\sigma = \sigma_\mathrm{inel}$. Thus, by mapping the measured antiparticle-to-particle ratio onto the fitted dependence to find the intercepts, the corresponding value of the inelastic cross section is found from the x values of the intercepts. 

\begin{figure}[htpb]
    \centering
    \includegraphics{}
    \caption{Bin by bin plots of the $^3\overline{\mathrm{He}}/{^3\mathrm{He}}$ ratio as a function of the varied inelastic cross section in Monte Carlo simulations, together with the one measured in data.}
    \label{fig:Meth:RatiosAsFunctionsOfSigmaInel3He}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics{}
    \caption{Bin by bin plots of the $^3\overline{\mathrm{H}}/{^3\mathrm{H}}$ ratio as a function of the varied inelastic cross section in Monte Carlo simulations, together with the one measured in data.}
    \label{fig:Meth:RatiosAsFunctionsOfSigmaInel3H}
\end{figure}

For the TPC/TOF method, a similar method is used in order to extract the measurement of the inelastic cross section from the TPC/TOF ratio. However, due to the increased amount of material budget which particles have to traverse and the much reduced statistical uncertainties provided by the Pb--Pb data set, the exponential functions are much less steep in the area of interest. An example of such a fit is shown in figure \ref{fig:TPCTOF_fit}. 

\begin{figure}[bthp]
    \centering
    \includegraphics{}
    \caption{TPC/TOF ratio for \ahe\ as a function of the varied inelastic cross section. Figure taken from \ref{}.}
    \label{fig:TPCTOF_fit}
\end{figure}
\subsubsection{Bench-marking the method on the antiproton inelastic cross section}
In order to be sure that the antiparticle-to-particle methods gives an accurate measurement of the inelastic cross section, it first had to be benchmarked to a particle for which the inelastic cross section was well known. This was done for antiprotons in \cite{antideuteronXS}. The resulting inelastic cross sections are shown in figure 

\begin{figure}
    \centering
    \includegraphics{}
    \caption{The antiproton inelastic cross section on the average ALICE detector material }
    \label{fig:my_label}
\end{figure}

talk about: 
 - show pbar results (from dbar paper)
 - how this validates the method (antiparticle-to-particle)
 - comment on the TOF/TPC method not having been benchmarked before? This provides a strong reason to do the antiparticle-to-particle method
\subsubsection{Non-linear error propagation}
The experimental observable is the antiparticle-to-particle ratio, which is then used to calculate the inelastic cross section. Thus, it is necessary to propagate the errors from the former to the latter. This is however non-trivial, since the two are related via an exponential as described in the previous section. Thus, the initially symmetric uncertainties on the ratio become asymmetric when propagated to the inelastic cross section. Additionally, the systematic and statistical uncertainties -- which are independent on the ratio and thus sum up in quadrature -- can no longer be summed in quadrature on the inelastic cross section, since the scaling between them is no longer linear but exponential. Indeed, as the slope of the exponential is not known a priori, the uncertainties cannot be added at all without knowledge of the dependence of the antiparticle-to-particle ratio in a given bin on the inelastic cross section. This leaves two options for the representation of uncertainties on the inelastic cross section: i) show the statistical and systematic uncertainties separately, and give the parameterization of the exponential curve used to add them together for each bin or ii) sum the two uncertainties on the ratio and then propagate the total uncertainty. The second option is significantly more practical since it gives the reader immediate access to the total uncertainty, and does not require extra explanation. The separate uncertainties can be recovered using the fits shown in figures \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3He} and \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3H}. \\

An important note is that this asymmetry arises far more prominently in the antiparticle-to-particle analysis than in the TOF/TPC analysis. This is due to 2 factors: the much reduced statistical uncertainties for the TPC/TOF analysis and the increased material budget required (as opposed to the TPC only part of the antiparticle-to-particle analysis). This results in the fact that within the uncertainties, the effect of the inelastic cross section on the TPC/TOF ratio is well approximated with a linear function. Thus, the error propagation from the TPC/TOF ratio to the inelastic cross section has only barely noticeable asymmetries. 

%todo: do the anlytical error propagation when the two parameters are related by an exponential. 
\subsubsection{Accounting for energy losses between the primary vertex and the point of annihilation}
We collect the histograms leading to the antiparticle-to-particle ratio as functions of the momentum these particles have at the primary vertex. However, the cross section should be given as a function of the momentum which the particles have during annihilation. This means energy losses which occur before annihilation need to be accounted for. Since we do not see tracks for particles which annihilate, this cannot be done on a case-by-case basis, but must be done statistically.\\

In order to correct for this, let us consider 
\subsubsection{Uncertainty coming from the material budget}
The measurement outlined in this thesis relies on the accurate knowledge of the ALICE material budget. This is because the loss of antinuclei is proportional to $e^{-\sigma_\mathrm{inel} \rho l}$, where $\rho$ is the density of the material traversed and $l$ is the path length of the antinucleus through the material. Thus, the material budget can be quantified as the sum of $\rho_i l_i$, over all the materials $i$ in the detector. This means that the constraints on the cross section are actually on the product of the cross section and the material budget, and this any uncertainty on the material budget is 1:1 applied to the inelastic cross section measurement. \\

Originally, the uncertainty on the material budget was quantified to be about 5\% using photon conversions\cite{}, up to the middle of the TPC (since later conversions would result in tracks which have less than half of the TPC clusters and thus cannot we well identified). However, this method left out the bulk of the material budget considered in any analysis using the TOF, as can be seen from figure \ref{fig:ALICE_detector_material_budget}. Therefore, the material between the TPC and TOF detectors needed to be validated using a different method. In order to to this, the same underlying idea as the TPC/TOF analysis was used, but rather than assuming a well known material budget and measuring the cross section, a particle with an accurately measured cross section was used in order to probe the material budget. The trick was to find such a particle which could be identified cleanly enough in the TPC alone. For this purpose, $\pi^+$ and $\pi^-$ from $\mathrm{K}_s^0 \rightarrow \pi^+ + \pi^-$ decays and protons from $\Lambda$ or $\overline{\Lambda}$ decays were used\cite{ALICE-PUBLIC-2022-002}. Due to their decay topology, they could be cleanly identified in the TPC alone, and their cross section was very accurately known. The measured ratio was then compared to ratios from simulation with varied ALICE material budget, in order to ascertain the uncertainty on the material budget between the TPC and TOF. The resulting uncertainties are shown in figure \ref{fig:ALICE_mat_budget}. It can be seen that an uncertainty of $\approx$ 5\% is achieved using this method. \\

\begin{figure}[bthp]
    \centering
    \includegraphics[width=\textwidth]{figures/ALICE_material_budget_tests.png}
    \caption{Uncertainty on the ALICE material budget between the TPC and TOF detectors, as found by comparing the yields in the TPC to the ones in the TOF for pions from $\mathrm{K}_s^0$ decays (red) and protons from $\Lambda$ decays (black). The uncertainty is determined by comparing the measured ratio to ones obtained from detailed Monte Carlo simulations of the ALICE detector with varied material budgets, using Geant3 (left) and Geant4 (right).}
    \label{fig:ALICE_mat_budget}
\end{figure}

Therefore, a global uncertainty of about 5\% is assumed on the material budget. The way this uncertainty is applied to the antiparticle-to-particle ratios is by means of the uncertainty due to the primordial ratio, which is the ratio of how many antiparticles are produced in respect to their particle equivalents, at the energies probed. This measurement is based on the $\bar{p}/p$ ratio as measured by ALICE \cite{}. The uncertainty on this ratio is 1.5\%, which is applied for each nucleon in question for both analyses, i.e. 3\% for antideuterons and 4.5\% for \ahe\ \atrit .
%\subsubsection{Evaluating the average material for antinuclei annihilations in the ALICE detector}
\subsubsection{Independence of collision system}
The antimatter-to-matter ratio method's dependence on collision system has been investigated by redoing the analysis performed in pPb collisions in \cite{dbarIvan} for high multiplicity pp collisions. The dependence on the collision system is due to the multiplicity differences, and the resulting difference in the baryochemical potential as discussed in section \ref{sec:MCSim}. By taking the antiproton-to-proton ratio for the different collision systems and comparing them, the predicted difference between the antideuteron-to-deuteron ratio was obtained. The results are shown in figure \ref{fig:pp_pPb_dbardRatio}, which show that the differences between collisions systems are consistent with the expected deviation. This independence of the collision system is expected, since the inelastic cross section is completely independent on the collision system. This becomes especially self-evident when considering that the annihilations do not occur in the initial collisions, but rather as the antiparticles travel through the detector material.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/antiproton-to-proton-ratio-pp-pPb-comparison.png}
    \includegraphics[width=0.49\textwidth]{figures/antideuteron-to-deuteron-ratio-comparison-pp-pPb.png}
    \caption{Ratio of the antiproton-to-proton ratios (left) and antideuteron-to-deuteron ratios (right) obtained in high multiplicity pp collisions and in pPb collisions, compared to the expected difference from the different baryochemical potentials (dashed red line).}
    \label{fig:pp_pPb_dbardRatio}
\end{figure}

