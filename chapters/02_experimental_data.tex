\section{Experimental data and experimental method}\label{sec:ExperimentAndMethod}

\subsection{ALICE}
This section aims to highlight the capabilities of the ALICE detector, in particular in the context of identifying antinuclei and measuring their inelastic cross sections. Measuring antinuclei inelastic cross section was not considered in the design of the ALICE detector. Rather, the excellent tracking and particle identification capabilities of the detector enable these measurements, which go beyond the scope originally envisioned for the detector. We shall therefore discuss the full chain of experimental methods, starting from the particle identification in each of the detectors, to the ALICE data structure and how they are used to obtain the antiparticle-to-particle ratios. 

\subsubsection{Overview}
ALICE is one of the four major experiments at the large hadron collider (LHC) near Geneva, Switzerland. It is the only dedicated heavy-ion experiment at the LHC, with its main physics motivation being the study of the quark-gluon-plasma (QGP). The experiment has 19 subdetector systems \cite{ALICE_overview}, of which the most important for this analysis are the Time-Projection-Chamber (TPC), the Inner Tracking System (ITS) and the Time-of-Flight detector (TOF). In particular the TPC sets ALICE apart from the other major LHC experiments, by enabling very precise tracking of particles, good particle identification via momentum and specific energy loss measurements, and its sensitivity low momentum particles (down to $p_T \approx $ 0.2 GeV/$c$). In particular the TPC is special because it maintains the capability to do all this in an environment with more than 10k charged tracks at mid-rapidity in central Pb--Pb collision. The momentum measurement is enabled by a solenoid magnetic field, which is usually operated at 0.5 T\footnote{There are also dedicated low B field runs, where the field is set to only 0.2 T.}. The central detector systems are constructed in a cylindrical shape, providing full azimuthal coverage. The coverage in the forward and backward direction can conveniently be described using the measure of pseudorapidity ($\eta$), which is defined as $\eta (\theta) = -\mathrm{ln}[\mathrm{tan}(\theta/2)]$, where $\theta$ is the angle which the emitted particle has to the beam axis. This measure is anti-symmetric around $\theta = \pi/2$, i.e. the angle normal to the beam axis, and is 0 at this angle. The central detectors of ALICE cover the midrapidity range of $|\eta|\lesssim 1$. A schematic representation of ALICE with all its subdetector systems can be found in figure \ref{fig:ALICE}. It is important to note that the detector discussed in this section is the ALICE detector as it existed during the Run 2 data taking period (2015-2018). The main limitation of this version of the ALICE detector was its low interaction and data readout rates, the latter of which was limited to about 1kHz in Pb--Pb collisions and 200kHz in pp collisions. This is due to the 1$\mu s$ timespan required for reading out a single event, which is dead-time for the detector. The ALICE upgrade for LHC Run3 which has started in 2022 will instead be able to read out data at rates up to 50 kHz, and provide a significant boost to the statistics which the ALICE detector can provide.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/2017-May-11-ALICE_RUN2_labels_HD.png}
    \caption{Schematic representation of the ALICE detector and its subdetector systems, during the Run 2 data taking period (2015-2018).}
    \label{fig:ALICE}
\end{figure}

\subsubsection{The ALICE Trigger System}
The ALICE detector cannot read out all events which occur, both due to the detector dead-time when reading out an event and due to the data rates which would be involved. Instead, interesting events are selected by predefined criteria, and then triggered upon. This trigger system then initiates the entire read-out sequence of the detector. The most basic trigger is the so called minimum bias (MB) trigger, which should trigger in the presence of any beam--beam collisions and not introduce any bias based on the occurring physics. This is an important data sample to check against, however, it is interesting to bias the selected events in favor of "more interesting" physics. Such triggers look for less common conditions more favorable to rare physics events, such as e.g. the presence of more charged particles. These triggers exploit the fact that data acquisition is limited by the ALICE read-out rate -- not by the occurrence rate of rare events -- in order to collect data for rare events at the same 200kHz (1kHz) rate  in pp (Pb--Pb) collisions at which MB data can be collected. \\

For pp collisions, the trigger-criterion used in the analyses presented in this thesis is the multiplicity in the V0 detectors (V0A and V0C), which is correlated with the charged particle multiplicity at midrapidity. The V0 detectors are plastic scintillator arrays in the forward and backwards regions, covering a pseudorapidity range of $2.8<\eta<5.1$ and $-3.7<\eta<-1.7$, respectively. They are located 3.4 m and 0.9 m from the interaction point. The high multiplicity trigger is configured so that the highest 0.17\% of multiplicity events are selected by the V0 detectors, while also requiring a minimum of 1 charged particle at midrapidity. This proxy works well for high multiplicities also at midrapidity, as this selection results in an average of 30-40 charged particles at mid-rapidity, as opposed to $\approx7$ particles for MB collisions. 

\subsubsection{Inner Tracking System (ITS)}
The Inner Tracking System (ITS) is the innermost detector in ALICE, staring at a radius of just 3.9 cm from the interaction point and reaching a radius of 43 cm. It consists of 3 lightweight silicon bases sub-systems, called the Silicon Pixel Detector (SPD), the Silicon Strip Detector (SSD) and the Silicon Drift Detector (SDD). The ITS covers a pseudorapidity range of $|\eta|<0.9$\footnote{The SPD can detect particles with a wider range, up to $|\eta|<1.95$.}. A schematic of the ITS is shown in figure \ref{fig:ITS_schematic}. Since the ITS is the closest detector to the interaction point, it plays a vital role in determining the position of the initial vertex of the collision, called the primary vertex. Indeed, its ability to accurately reconstruct particle trajectories enables the reconstruction of the primary vertex to a precision of 100 $\mu$m, and constrains the particles' distributions of their distance-to-closest-approach (DCA). This is particularly important when analysing nuclei at low momenta, since for deuterons, $^3\mathrm{He}$ and $^3\mathrm{H}$ the contribution from secondaries from material spallation is the dominant contamination in the nuclei signal. The requirement of a cluster in the first ITS layer (SPD), removes any tracks from particles which get created from material interactions at larger radii, unless there is a matchable cluster on their trajectory by chance. The ITS also allows the rejection of pile-up events\footnote{Pile-up is what happens when tracks from a different physical collision are incorrectly matched to the same event.}. The particle identification capabilities of the ITS become less reliable for very large specific energy loss ($dE/dx$) due to saturation effects, which makes the ITS less useful for PID of doubly charged particles such as $^3\mathrm{He}$.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ITS_figure.jpeg}
    \caption{A schematic of the ALICE Inner Tracking System. The three layer groups (SPD, SSD, SDD) are marked. }
    \label{fig:ITS_schematic}
\end{figure}

\subsubsection{Time Projection Chamber}
The Time Projection Chamber (TPC) is the main tracking detector of the ALICE experiment. It follows the ITS in the central barrel, at radii from 85 cm to 247 cm from the interaction point, covering a pseudorapidity range of $|\eta|<0.9$. The schematic layout of the TPC is shown in figure \ref{fig:TPC_schemtic}. It consists of a gas filled field cage, which can measure the ionisation caused by charged particles travelling through the gas. Due to the applied  electric field, the electrons created from ionisation drift towards the read out cathodes of the field cage. The amplitude of the measured signal then gives a measure of the specific energy loss of the particles (dE/dx), while the position of the clusters at the readout cathode gives the 2-dimensional (x and y) position of the tracks of the particles. Finally, by measuring the time of arrival of the electrons relative to the timing of the initial collision, the z position of the clusters can be calculated. This method is shown on the right of figure \ref{fig:TPC_schemtic}. Measuring the position and therefore the curvature of the track allows the determination of the momentum of the particles. 

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/tpc_cage.png}
    \includegraphics[width=0.49\textwidth]{figures/tpc_process.png}
    \caption{Left: Schematic of the field cage of the TPC detector \cite{TPC_figure}. Right: Schematic of the reconstruction mechanism for tracks in the TPC \cite{ALICE_TPC_trigger}.}
    \label{fig:TPC_schemtic}
\end{figure}

Due to the combination of a momentum and a specific energy loss measurement, the TPC has excellent particle identification abilities. The energy loss of relativistic particles\footnote{At very low energies below $\lesssim 0.5 $MeV and at very high energies $>100$GeV, the Bethe-Bloch formula does not apply. } is given by the Bethe-Bloch formula, which is reproduced in equation \ref{eq:Bethe-Bloch}:

\begin{equation}\label{eq:Bethe-Bloch}
    -\frac{dE}{dx} = \frac{4\pi n z^2}{m_e c^2 \beta^2} \left( \frac{e^2}{4\pi \epsilon_0}\right)^2 \left[ \mathrm{ln}\left(\frac{2m_ec^2\beta^2}{I(1-\beta^2)}\right)-\beta^2 \right]
\end{equation}
, where $n$ is the electron density of the material, $I$ is the mean excitation energy of the material, and the other symbols have their usual meaning. Since equation \ref{eq:Bethe-Bloch} is a function of only $\beta$ and $z$ for a given material, a measurement of both the momentum $p=\frac{\beta}{\sqrt{1-\beta^2}}m$ and the energy loss will differentiate particles of different masses. This separating power of the TPC is shown in figure \ref{fig:PID_TPC}, which shows that at low momenta, particles are very well identified by the TPC alone. \\

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/Ali_TPC_performance_pp.png}
    \caption{Specific energy loss in the TPC as a function of the rigidity $p/z$. Due to their masses, particles can be differentiated according to equation \ref{eq:Bethe-Bloch}. This shows the identifying power of the TPC for low momentum particles.}
    \label{fig:PID_TPC}
\end{figure}

The TPC Particle identification (PID) response can be be expressed as the variable $n\sigma_{\mathrm{TPC}}$, which is a measure of how close a track follows the Bethe-Bloch curve of a given particle hypothesis, according to equation \ref{eq:nsigma}:

\begin{equation}\label{eq:nsigma}
    n\sigma_{\mathrm{TPC}} = \frac{(\frac{dE}{dx})_{meas} - <\frac{dE}{dx}>_{exp}}{\sigma_{resolution}}
\end{equation}
, where $\frac{dE}{dx}$ is the specific energy loss at a given momentum and $\sigma_{resolution}$ is the resolution of the TPC. 

\subsubsection{Time-of-flight detector (TOF)}

As can be seen from figure \ref{fig:PID_TPC}, the differentiating power of the TPC decreases drastically at higher momenta, as the energy loss of particles tends towards the value for a minimum ionising particle (MIP), and their bands thus start overlapping with each other. In order to differentiate between particles of different masses at higher momenta, an additional information is required. The detector used for this purpose in ALICE is the Time-of-flight (TOF) detector. The TOF is a detector based on multigap resistive plate chambers \cite{ALICE_TOF_TDR}, which measures the time difference between the initial collision and the formation of a cluster in one of its readout pads. It is arranged in a cylindrically symmetric structure between 370 cm and 399 cm from the interaction point and has the same coverage in pseudorapidity as the TPC ($|\eta| < 0.9$). The TOF is mounted in a steel structure called the space frame\cite{ALICE_TOF_TDR, ALICE_overview}. A schematic representation of the TOF detector is shown in figure \ref{fig:TOF_schematic}. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/TOF.png}
    \caption{The TOF detector of the ALICE experiment.}
    \label{fig:TOF_schematic}
\end{figure}

The time resolution of the TOF readout pads is $\approx 50$ ps\footnote{In order to get an idea of this resolution, a particle travelling at the speed of light will travel roughly $c*50 \mathrm{ps} = 1.5$ cm.}. In order to measure the time-of-flight, the initial time of the collision must be known. This can be done by the TOF itself with large enough multiplicities, and for lower multiplicities it is done with the T0 detector, which consists of Cherenkov arrays.
Thus, a measure of the particle velocity, called the TOF beta, can be measured as $\beta = L/t$, where $L$ is the length of the track on its curved trajectory through the TPC, and $t$ is the measured time-of-flight. From the relation $p = \gamma \beta mc$, equation \ref{eq:TOFm2} can be derived, which relates the measured $\beta_{\mathrm{TOF}}$ to the tracks mass. The factor $1/Z^2$ cannot be neglected here since the detector cannot know the particles mass a piori. Thus, when analysing multicharged particles such as \ahe\ , the observable from the TOF is $m_{\mathrm{TOF}}/Z^2$. 

\begin{equation}\label{eq:TOFm2}
    m^2/Z^2 = \frac{p^2}{c^2 Z^2}\left(\beta_{\mathrm{TOF}}^2\gamma^2 \right) = \frac{p^2}{c^2 Z^2} \left( \frac{c^2t^2}{L^2} -1\right)
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/2016-Sep-08-beta.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/anti3he_tofm2_seminar.png}
    \caption{Left: Measurement of the track velocity $\beta_{\mathrm{TOF}}$ from the TOF detector in pp collisions at $\sqrt{s}=13$ TeV. $\beta_{\mathrm{TOF}}$ is classically measured as length of the track divided by the time-of-flight. Right: Performance figure showing the TOF used for the identification of \ahe\ nuclei.}
    \label{fig:TOF_performance}
\end{figure}

The performance of the TOF detector is shown in figure \ref{fig:TOF_performance}. A clear separation between particles can be seen up to much higher momenta than in the TPC. This is particularly true for higher mass particles. The deuteron line can be seen below the marked proton line, and is well differentiable well beyond the merging of the proton and pion lines. 



\subsubsection{Basics of ALICE data structure}
ALICE data is split by periods, which in turn consist of runs, then by events, and within the event by tracks, as is shown in figure \ref{fig:ALICE_data_schematic}. Runs are the periods of time during which collisions with the same conditions occurred, which means that the data taking is started and kept up until either the LHC beam cycle comes to an end or there is some problem which requires the run to be ended. This means that runs are of arbitrary length. Once the raw data is taken, the Data Preparation Group (DPG) is responsible for doing a reconstruction pass over the data, which means to build the tracks from the individual detector hits, correcting for any calibration or distortion effects. The data structure  one is left with is a list of events, each of which contain a list of tracks. This is what is subsequently used by analysers\footnote{There are two files available for runs: ESD and AOD files. The difference is the level of lossy compression in each track. ESD files keep more information -- such as a track's momentum at different points in the TPC -- while AOD files are faster to analyse due to the smaller memory required.}. This hierarchy is shown in figure \ref{fig:ALICE_data_schematic}.

\begin{figure}
	\includegraphics[width=\textwidth]{figures/data_structure.png}
	\centering
	\caption{Schematic of the data structures within ALICE. The data is split by run periods, then by event, and within the event by tracks. }
		\label{fig:ALICE_data_schematic}
\end{figure}
\subsection{Identifying antinuclei and building the antiparticle-to-particle ratio}
This section described the process to identify (anti)nuclei using the ALICE detector, and the method by which the antiparticle-to-particle ratio is then reconstructed. For this purpose, $10^9$ high multiplicity pp events at $\sqrt{s}$ = 13 TeV were analysed. 
\subsubsection{Collision system and event selection}
The data provided in ALICE by necessity includes a large range of particles. For analyses which do not want all charged particles, these act as impurities. Therefore, cuts are applied at the analysis level, to provide a much cleaner environment for the actual analysis. Within the analysis, these cuts happen on both an event and a track level, leaving a subset of tracks which can be analyzed. The goals of these cuts are: i) to cut bad quality tracks, such as ones where the PID is not certain, ii) to cut tracks of uninteresting particles for the specific analysis, e.g. particles produced by material spallation in the analyses in this thesis and iii) to reduce the background, such as from secondary particles from weak decays. These cuts also vary between collision systems, which is necessitated by their different properties. To exemplify this, lets compare a relevant difference between high multiplicity pp and Pb--Pb collisions. In HM pp collisions the mean multiplicity is 34, while in central Pb--Pb collisions it is about 1000. This means that the mean occupancy of the detectors is much greater in Pb--Pb collisions, which in turn means that the tracking algorithm has a higher chance to assign a wrong cluster to a track. In order to reduce this effect, the matching window for the TOF detector is reduced in Pb--Pb collisions, from 10 cm to 3 cm. For the analysis method explained in section \ref{sec:TOFTPCMethod}, this introduces an uncertainty, as some tracks could be elastically scattered in the TRD or space frame, causing them to miss the matching window without having interacted inelastically.  To evaluate and counteract this, a special reconstruction of the Pb--Pb data was used, where the matching window was set to 10 cm instead of 3 cm. The effect of this change is explained in section \ref{sec:TOFTPCMethod}. \\

\subsubsection{Reconstruction of raw (anti)nuclei spectra}\label{sec:Meth:PIDandSelections}
In order to obtain the raw antinuclei spectra, the tracks first have to be identified as antinuclei. This particle identification (PID) occurs on the basis of two main detectors: the TPC and the TOF. Due to the distinct masses of antinuclei (they are heavier than most other long lived particles), they leave a distinct signal in each detector. In the TPC, antideuterons are clearly separated by their energy loss up to a momentum of about 1.4 GeV. \ahe\ is well separated from lighter particles in the TPC for all momenta, due to its double charge\footnote{This means that \ahe\ has to contend with impurities from $^4\overline{\mathrm{He}}$, which is also doubly charged. However, given that for each additional nucleon a penalty factor is introduced for the production (as shown in figure \ref{fig:PenaltyFactorNuclei}), this contribution is below the \% level and therefore negligible with the uncertainties of this analysis.}. Since the energy loss rises with $Z^2$, the energy loss of \ahe\ is characteristically much higher than those of singly charged particles. The particle identification of \atrit\ uses the TOF for all considered momenta.\\
In the TOF, the time of flight measurement combined with the track length and curvature gives a measurement of the particles mass, according to equation \ref{eq:TOFm2}. This allows a clean signal for \atrit\ and \ahe\ . For antideuterons, there is still a significant contamination from the tail of the proton distribution at those masses, which requires a fit to the signal and the background to extract the antideuteron yield. Figures \ref{fig:he3_TPC_PID} and \ref{fig:he3_TOF_PID} show the extraction procedure in the TPC and TOF for \ahe\ . Figure \ref{fig:h3_TPC_PID} shows the extraction for \atrit\ . The particle and antiparticle $n\sigma_{\mathrm{TPC}}$ distributions are fit with a gaussian function. For \ahe\ , a second gaussian is used to account for the background from (anti)triton\footnote{The reason why the contamination shows up in the low momentum bins for helium but not for tritons is due to the double charge of \ahe\ . This means that by grouping particles in bins of measured momentum, we are actually grouping them in bins of $p/Z$. Thus, when looking at a given bin in $p/Z$, tritons have half the momentum of \ahe\ . Since this contamination is at values of $p/Z$ before the start of the triton analysis, the inverse contamination does not need to be corrected for in the \atrit\ measurement. }. Both the \ahe\ and \atrit\  signals in the TOF detector are very clean, as is shown in figures \ref{fig:he3_TOF_PID} and \ref{fig:h3_TPC_PID}, therefore, the TOF signal is used by applying a cut on the $m_{TOF}^2$.  The combination of these measurement allows the extraction of the (anti)nuclei spectra, which are shown in \ref{fig:(anti)nucleiSpectra}. The combined signal extraction cuts are shown in table \ref{tab:PIDcuts}.
\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/0.65<p<0.8_AHe3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/0.65<p<0.8_He3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/0.8<p<1.0_AHe3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/0.8<p<1.0_He3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \caption{Particle identification procedure for \ahe\ (left) and $^3\mathrm{He}$ (right), showing the distribution of $n\sigma_{\mathrm{TPC}}$ for the momentum bins in the TPC only part of the analysis. The green line is the fitted signal, the red line is to fit the contamination towards negative $n\sigma_{\mathrm{TPC}}$. The black line shows the combined fit.}
    \label{fig:he3_TPC_PID}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/1.0<p<1.2_AHe3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/1.0<p<1.2_He3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/1.2<p<1.5_AHe3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/1.2<p<1.5_He3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \caption{Plots of the $n\sigma_{\mathrm{TPC}}$ distribution for \ahe\ (left) and $^3\mathrm{He}$ (right), for the momentum bins in the TPC+TOF only part of the analysis, i.e. after a cut on $m_{\mathrm{TOF}}^2$ is applied. The green line is the fitted signal.}
    \label{fig:he3_TOF_PID}
\end{figure}

\begin{figure}\ContinuedFloat
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/1.5<p<2.0_AHe3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/1.5<p<2.0_He3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/2.0<p<4.0_AHe3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \includegraphics[width=0.48\textwidth]{figures/Selected_Plots/2.0<p<4.0_He3_TPCnSigma_binsize_nS=0.5_rebin_DCA=1_pVtx.png}
    \caption{(Continued) Plots of the $n\sigma_{\mathrm{TPC}}$ distribution for \ahe\ (left) and $^3\mathrm{He}$ (right), for the momentum bins in the TPC+TOF only part of the analysis, i.e. after a cut on $m_{\mathrm{TOF}}^2$ is applied. The green line is the fitted signal.}
    \label{fig:he3_TOF_PID}
\end{figure}


\begin{figure}
    \centering
    %\includegraphics[width=0.48\textwidth]{figures/triton/ATriton_TPCnSigma_Bin1.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/ATriton_TPCnSigma_Bin2.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/Triton_TPCnSigma_Bin2.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/ATriton_TPCnSigma_Bin3.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/Triton_TPCnSigma_Bin3.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/ATriton_TPCnSigma_Bin4.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/Triton_TPCnSigma_Bin4.png}
    \caption{Particle identification procedure for \atrit\ (left) and $^3\mathrm{H}$ (right), showing the $n\sigma_{\mathrm{TPC}}$ distribution for each momentum bin, after a cut on $m_{\mathrm{TOF}}^2$ is applied.}
    \label{fig:h3_TPC_PID}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/he3_spectra.png}
    \includegraphics[width=0.48\textwidth]{figures/}
    \caption{(Anti)nuclei spectra for $^3\mathrm{He}$ \ahe\ (left) and $^3\mathrm{H}$ \atrit (right). These spectra are not yet corrected for secondary particles from material spallation.}
    \label{fig:(anti)nucleiSpectra}
\end{figure}


\begin{table}
%todo insert table of PID cuts from analysis notes.
\end{table}

It is important to note that the histograms in this analysis are low statistics histograms, i.e. they have many bins with 0 counts towards their sidebands. This presents challenges when using the default implementations of $\chi^2$ fitting algorithms, since those tend not to treat empty bins rigorously, if they are included in the fit at all. Therefore, a minimised log-likelyhood fit was done, using proper Poisson errors on empty bins (i.e. empty bins are assigned an uncertainty of $\pm 1.14$, for further information see the statistics chapter in \cite{PDG2022}). For a more detailed comparison of different fitting algorithms, see section \ref{sec:App:lowStatFitting} in the appendix.

%todo: write a paragraph and insert figures on different fitting algorythms in AliPhysics, what they do and how this affects low statistics analyses -> i.e. a lot. Also talk about errors on the 0 bins here. 


\subsubsection{Correction for secondaries from material spallation}\label{sec:Meth:secondaryCorr}
In order to obtain pure samples of nuclei, any secondary nuclei not created in the initial collision need to be subtracted from the obtained raw spectrum. Two sources of secondary particles exists: weak decays and material spallation. For \ahe\ and \atrit\, weak decays are negligible, since the amount of $^3\mathrm{H}_\Lambda$ measured in pp collisions is much less than the amount of \ahe\ . The branching ratio of $^3\mathrm{H}_\Lambda \rightarrow $ \ahe\ is expected to be 25\% \cite{PDG2022}. Thus, secondary nuclei from material spallation remain, which shall simply be referred to as secondaries hereinafter. Since these secondaries are created by essentially "knocking out" these nuclei from larger nuclei in the ALICE detector material and in the beampipe, no secondary antinuclei exist. In order to differentiate between secondaries and primaries, we make use of the fact that all primaries have a common origin (the primary vertex), while the distribution of secondaries should not point to the primary vertex. The measure of how close a particle's track reaches to the primary vertex is known as the distance of closest approach (DCA), and within ALICE is resolved in both the $xy$ and the $z$ planes. In order to fit these distributions, they have to be projected onto one of the two directions, which involves cutting on the other. The DCA$_{xy}$ distributions were used for the fits described later in this chapter. The resulting changes in the primary $^3\mathrm{He}$ yields depending on the values of DCAz which are selected is shown in figure \ref{fig:DCAzcutEffect}. An uncertainty of 8\% is applied to the first bin as a result of this cut.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/Effect_of_DCAz_cut.png}
    \caption{Extracted primary $^3\mathrm{He}$ yields in each analysis bin as a function of the value of the cut on |DCAz|. Due to the variations in the first bin, an 8\% uncertainty was assigned.}
    \label{fig:DCAzcutEffect}
\end{figure}

We expect the primary DCA distribution to be peaked sharply at 0, while the distributions for secondaries should be mainly flat. Example distributions from Monte Carlo Simulations are shown in figure \ref{fig:ExampleDCADistributions}.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/Example_templates.png}
    \caption{Example DCA$_{xy}$ distributions of particles from primary and secondary particles in Monte Carlo simulations. The  particle shown here is \ahe\ with a |DCA$_z$|<1 cm requirement. }
    \label{fig:ExampleDCADistributions}
\end{figure}

%todo include figure for exemplary DCA distributions 
Figure \ref{fig:ExampleDCADistributions} shows that while the distribution for primaries is indeed sharply peaked at 0, the distribution of secondaries is not flat, but also peaks around 0. This is an experimental effect due to the tracking algorithm, which prefers reconstructing tracks pointing towards the primary vertex. This is exacerbated by the possibility to assign a wrong ITS cluster to the track. Several cuts can be made on the tracks to minimize this effect, which are outlined in section \ref{sec:Meth:PIDandSelections}. The most important cut is on the number of clusters in the first ITS layer, which reduces the number of secondary tracks by $\approx$ 85\%, as is shown in figure \ref{fig:DCA_dist_ITS_cut_effect}. 
\\

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/DCA_dist_3he_no_cut.png}
    \includegraphics[width=0.48\textwidth]{figures/DCA_dist_3he_ITSNClusters_cut.png}
    \caption{DCA$_{xy}$ distributions of $^3\mathrm{He}$ candidates without any cut on ITS hists (left) and after a hit in one of the two first layers of the ITS is required (right). The reduction in the number of candidates is mainly in the sidebands, and therefore from secondaries.}
    \label{fig:DCA_dist_ITS_cut_effect}
\end{figure}
The biggest challenge with secondary corrections is to get reliable templates for secondary nuclei from material. For a combinatorial background, a side band analysis can be done, since no deviating behaviour in the signal region is expected, but since we have already seen in figure \ref{fig:ExampleDCADistributions} that secondary tracks are also peaked towards the primary vertex, the sideband analysis cannot help us account for this. It is however also impossible to extract a pure secondary distribution from data, since the peak region always necessarily includes the particles produced in the initial collision. Thus, we need to simulate the distribution with Monte Carlo simulations. This means that we rely on the assumption that all the involved processes are accurately reproduced in Monte Carlo, specifically, we rely on the fact that the angular distribution of the spallation processes are accurately reproduced\footnote{The absolute value of the cross section is not important for the accuracy of the templates, since the relative weight is later determined by the template fits. However, too low a cross section means that far more events have to be simulated in order to gain sufficient statistics to obtain the template.}. The advantage of this method, is that in full ALICE Monte Carlo simulations, the same tracking algorithm is used as in data reconstruction, which means that if the spallation processes are accurately simulated, the distribution will match the true distribution. Also, these simulations rely on the correct underlying event, i.e. for high multiplicity pp collisions, such collisions need to be accurately simulated. This is due to the fact that the spallation is triggered by particles produced in the primary collision. A final challenge to obtaining the template fits is the rarity of these spallation processes in MC simulations. \\


In order to extract the secondary fraction from the DCA distributions, template fits are used. These fits take the shape of input templates (in this case from primaries and from secondaries) and try to match their relative contribution in order to reproduce the shape in data. Two different fitting algorithms exists for this purpose within the ALICE analysis framework: the TFractionFitter and Roofit. The main difference between the two is that the TFractionFitter can change the shape of the templates within uncertainties in order to better reproduce the data. In the limit of infinite statistics, both of these methods should produce the same result. In the analyses shown in this thesis, the TFractionFitter was used as the default method, and Roofit was used to crosscheck these results. Additionally, a sideband analysis of the templates was performed as an additional crosscheck. A comparison of the template fits obtained using these three methods is shown in figure \ref{fig:Template_Fit_methods_comparison}. As can be seen, the uncertainty introduced by the scaling of the histogram (the comparison of the left and central panels in figure \ref{fig:Template_Fit_methods_comparison}) is less than the uncertainty returned by the fit. The detailed fits and corresponding primary fraction are shown in sections \ref{sec:ResHe3SigmaInel} and \ref{sec:ResTritonSigmaInel}.\\   

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/comparison_template_fitting_techniques.png}
    \caption{Comparison of different methods for determining the primary fraction from the template fits, shown in the second bin of the \ahe\ analysis, with a |DCA$_z$|<1cm cut. (Left) Fit using the TFractionFitter. (Middle) Default templates scaled according to the weights assigned by the TFractionFitter, but without changing their shapes. (Right) Fits performed by scaling the material templates to the region outside |DCA$_{xy}$| < 0.1 cm. The solid line represents the data and the histogram points are the fitted material template.}
    \label{fig:Template_Fit_methods_comparison}
\end{figure}

%write about rarity of these processes, back it up with the general purpose calculations, write for the need for the special reconstruction, and compare Geant3/Geant4 distributions


One could also ask the question of which primary particles are responsible for the largest amounts of nuclei secondaries from spallation. In order to investigate this question, a toy Monte Carlo simulation was used, where beams of primary particles were fired on layers of beryllium and beryllium + carbon, corresponding to the materials of the beampipe and the support structure of the first ITS layers. This configuration was chosen since if the spallation occurs later, the missing hit in the first ITS layer allows a large degree of rejection\footnote{This is somewhat less true in Pb-Pb collisions, since the multiplicities are so much higher and therefore a wrongly associated ITS cluster is far more likely.}. An exponential energy spectrum was used for the primary particles, tuned to the proton spectrum measured by ALICE \cite{Aamodt:2010dx}. Geant4 was employed for this simulation\cite{GEANT4:2002zbu, Geant4_developments}. The resulting yields of secondary deuterons and $^3\mathrm{He}$ are shown in figure \ref{fig:toyMCGeant4Spallation}. Interestingly, the primary antiparticles produce a larger portion of the secondary nuclei than their particle partners. Also, while antideuterons produce a larger amount of secondary nuclei than antiprotons (by roughly 2x), given that their relative abundance in pp collisions is 1000x less, their contribution is expected to be on the sub \% level. This leads to the conclusion that it is mainly (anti)protons and pions which are responsible for creating secondary antinuclei. Therefore, when using ALICE Monte Carlo simulations, it is not necessary to employ a coalescence afterburner with the underlying event in order to accurately simulate the secondary distributions. A caveat to this is that the simple toy Monte Carlo simulation only probed absolute yields, rather than the angular distribution, and as already noted above, the latter is the important factor. However, given that the contribution to the yields is on the sub \% level, any difference in the distribution is expected to be negligible. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/simpleMC_secondaries_G4.png}
    \caption{Results of a simple toy Monte Carlo simulation of a particle beam on materials mimicking the LHC beampipe in ALICE and the beampipe + ITS support structure. The resulting secondary deuterons and $^3\mathrm{He}$ are shown as a function of the primary particle fired, where the results are roughly scaled by the primary particles relative abundance.}
    \label{fig:toyMCGeant4Spallation}
\end{figure}
\subsubsection{Annihilations within the detector}
Annihilations within the detector material can occur at any point within the detector, but are of course more likely in denser materials. For the purpose of this discussion we shall differentiate between 3 different scenarios: i) annihilations before the middle of the TPC, since such tracks cannot be identified and will therefore not be reconstructed in our spectra. ii) annihilations between the middle of the TPC and the TOF, since these annihilations can be directly probed by the comparison of the yields in the TPC and TOF. And finally annihilations outside of the TOF detector, which for the purposes of this analysis is not seen at all, i.e. such annihilations are not measured. \\

Let us first consider the case where the annihilation occurs before the middle of the TPC. A track with less than half of the TPC clusters will be removed by the track cuts, therefore this track will not show up in our analysis, and will never even be identified as an antinuclei candidate track. The situation is slightly different when considering tracks which annihilate between the TPC and the TOF. Those tracks can be identified in the TPC. For \ahe\ this identification can occur over the whole momentum range ($0.5<p/Z<4$ GeV/$c$ in HM pp collisions), while for antideuterons this identification only works up to $p<1.4$ GeV/$c$ and for \atrit\ it only works up to $p<1.5$ GeV/$c$. However, since the antinucleus does not reach the TOF, the TOF hit will either be missing, or at a wrong time (i.e. giving an incorrect TOF mass). This allows for two options in these analyses: in the case where the TPC is sufficient to clearly identify the antinucleus, and it is within the acceptance of the TOF, the difference between the TPC and TOF yields can be used in order to probe the antinuclei inelastic cross section without being reliant on the corresponding nuclei yields. The second option is to use the TOF information in order to increase the amount of material which the particles need to traverse before being considered in the analysis, which makes the ratio more sensitive to the inelastic cross section. This increase is rather drastic, since the material budget increases by a factor of $\approx 5$ when switching between a TPC only analysis and one which includes the TOF. This can be seen in figure \ref{fig:ALICE_detector_material_budget}, which shows the cumulative material budget in ALICE as a function of radius. \\

We are thus left with the two possible methods for measuring annihilations within our detector. The first is based on quantifying the loss of antiparticles as they move through the detector, by comparing them to their particle counterparts. This method works for any particles and momentum range which the detectors can probe a priori\footnote{As we will have seen in section \ref{sec:Meth:secondaryCorr}, the unreliability of the secondary correction at low momentum limits the low momentum reach of this method.}. As part of this thesis, this method was performed for \ahe\ and \atrit\ . The second is based on comparing the yields in the TPC and the TOF, in regions where the antinucleus can be clearly identified in the TPC alone, and which include the acceptance of the TOF. \\

The analyses utilising the TOF-to-TPC method for the measurement of the inelastic cross sections of \ahe\ and \atrit\ were performed by others, and are reproduced in this thesis since they are closely related to the results shown in this thesis. The measurement of the antideuteron inelastic cross sections in pp and p--Pb collisions were also not done as part of this thesis, however the comparison of the two results to show the independence of the antiparticle-to-particle ratio on the chosen collision system was performed as part of this thesis. 

\begin{figure}
		\centering
		\includegraphics[width=\textwidth]{figures/alice-material_0.png}
		\caption{Cumulative material budget of the ALICE detector, as a function of radius from the beampipe, taken from \cite{antideuteronXS}.}
		\label{fig:ALICE_detector_material_budget}
\end{figure}    

\subsection{Extracting the inelastic cross section from the antimatter-to-matter ratio}
The idea behind using the antimatter-to-matter ratio as the observable to measure the antinuclei inelastic cross section, is that antinuclei will annihilate in the detector material, and therefore disappear from our measurement\footnote{Annihilation of antinuclei is the dominant inelastic process at low energies, however, it is not the only process we observe. Antinuclei -- being composite objects -- may also break apart in inelastic reactions which leave the antinucleons intact. The measurement techniques described in this section measure the total inelastic cross section, which includes all inelastic processes.}. In order to quantify the inelastic cross section we thus need to know how many particles were originally produced, i.e. we need to normalise the antinuclei spectrum to the number of originally produced antinuclei. However, we cannot use theoretical predictions tuned to this data, since that would be a circular argument, i.e. we would get out the same inelastic cross section as we put in. Therefore, the matter nuclei are used as a proxy instead. This works very well for a few reasons. First, the matter inelastic cross section can be easily measured, and have been measured for deuterons \cite{deuteron_cross_section}, $^3\mathrm{He}$\cite{chargeradius_helium}. For $^3\mathrm{H}$, the inelastic cross section could be measured with the same method, but has not been measured yet. Second, other effects on the  acceptance or efficiency will largely cancel between the nuclei and antinuclei counterparts, since the two only differ in their charge sign. Third and perhaps most important, is the fact that at LHC energies, the primordial ratio is very close to unity, and has been accurately measured for antiprotons\cite{Abbas_2013_primordial_ratio}. This means that we know to a very high degree of accuracy how many antinuclei are produced relative to the produced nuclei, and the other processes by which both might be lost within the detector are also well understood. Thus, the antimatter-to-matter ratio is sensitive to the antinuclei inelastic cross section, and other variables it is sensitive to are well understood and under control. This makes this ratio such a promising probe to measure the inelastic cross section.\\

Having established that the antimatter-to-matter ratio is sensitive to the inelastic cross section, it is still not trivial to extract the inelastic cross section from this observable. This difficulty is due to having to account for many processes. One example is the path which the particles take through the detector. In the magnetic field, (anti)nuclei travel on curved tracks, so the amount of matter they interact with will depend on their initial trajectory. This thus needs to be averaged over the $\eta$ distribution of the antinuclei. This is just one of many similar effects which make an analytical relationship between the antimatter-to-matter ratio and the antinuclei inelastic cross section difficult to achieve. Fortunately, detailed simulations of the ALICE detector using Geant4 account for all pertinent interactions of (anti)nuclei. We therefore compare our measured ratios to ones obtained using Geant4 simulations, in order to obtain our results on the inelastic cross section. In order to probe the relationship of the antinuclei-to-nuclei ratio to the inelastic cross section, the Geant4 code was modified to vary the inelastic cross section, keeping all other interactions the same.
\subsubsection{Using the antipartilce-to-particle ratio from Monte Carlo simulations}\label{sec:MCSim}
In order to fairly compare the Monte Carlo simulations to the produced data, it is vital to account for the primordial ratio\footnote{In other words: how much more antimatter particles we have for each matter particle. Given that we collide purely matter particles, there is a penalty for producing antimatter, even though at such high energies it is vanishingly small.} at such high energies. The relevant ratio of antiprotons-to-protons is shown in figure \ref{fig:BaryochemicalPotential}. Based on the same arguments as the formula for the coalescence parameter \ref{eq:CoalescenceParameter}, the effect on the ratio of antinuclei will be the same as to the antiproton-to-proton ratio taken to the exponent of the mass number of the antinucleus. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/primordial_ratio.pdf}
    \caption{Ratio of antiprotons to protons produced at mid-rapidity as a function of beam rapidity. At LHC energies the value approaches unity, demonstrating that at such high energies antimatter and matter are produced in almost equal amounts. Figure taken from \cite{Abbas_2013_primordial_ratio}.}
    \label{fig:BaryochemicalPotential}
\end{figure}



\subsubsection{Ratios as a function of the inelastic cross section scaling factor}
In order to extract the inelastic cross section measurement from the antiparticle-to-particle ratio, we have to compare the measured ratio in each bin to values from MC simulation with varied values of the inelastic cross section. The use of MC is necessary in order to obtain the dependence of the inelastic cross section on the antiparticle-to-particle ratio. This then allows the bin-by-bin extraction of the inelastic cross section by comparing the dependence in MC to the measured value of the ratio in the data \footnote{This is a proxy for the actual inelastic cross section measurement. However, the mapping from the scaling factor to the inelastic is not exact, but is subject to the uncertainty from any energy loss before annihilation occurs, as will be discussed in section \ref{sec:Meth:ELoss}.}. These plots are shown for the $^3\overline{\mathrm{He}}/{^3\mathrm{He}}$ and $^3\overline{\mathrm{H}}/{^3\mathrm{H}}$ ratios in figures \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3He} and \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3H}, respectively. These plots also show fit lines to the Monte Carlo points, which were with with an exponential according to the Lambert-Beer absorption law \cite{Lambert-Beer}, which is reproduced in equation \ref{eq:LambertBeer}

\begin{equation}\label{eq:LambertBeer}
    N_{\mathrm{surv}} = N \times \mathrm{exp}\left( -\sigma \times \rho \times L \right)
\end{equation}
, where $L$ is the distance travelled through a medium, $\sigma$ is the absorption cross section and $\rho$ is the density of the medium. The only difference between the different Monte Carlo simulations is the implemented inelastic cross section, $\sigma = \sigma_\mathrm{inel}$. Thus, by mapping the measured antiparticle-to-particle ratio onto the fitted dependence to find the intercepts, the corresponding value of the scaling factor on the inelastic cross section is found from the x values of the intercepts. \\

\begin{figure}
    \centering
    %\includegraphics[width=0.32\textwidth]{figures/he3_rescaling/Bin_1_scaling.png}
    \includegraphics[width=0.32\textwidth]{figures/he3_rescaling/Bin_2_scaling.png}
    \includegraphics[width=0.32\textwidth]{figures/he3_rescaling/Bin_3_scaling.png}
    \includegraphics[width=0.32\textwidth]{figures/he3_rescaling/Bin_4_scaling.png}
    \includegraphics[width=0.32\textwidth]{figures/he3_rescaling/Bin_5_scaling.png}
    \includegraphics[width=0.32\textwidth]{figures/he3_rescaling/Bin_6_scaling.png}
    \includegraphics[width=0.32\textwidth]{figures/he3_rescaling/Bin_7_scaling.png}
    \caption{Bin by bin plots of the $^3\overline{\mathrm{He}}/{^3\mathrm{He}}$ ratio as a function of the varied inelastic cross section in Monte Carlo simulations, together with the one measured in data. The fitted line is an exponential fit according to the Lambert-Beer law \ref{eq:LambertBeer}, and is used to extract the cross section scaling factor.}
    \label{fig:Meth:RatiosAsFunctionsOfSigmaInel3He}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.32\textwidth]{figures/triton/tbar_Fit_Bin1.png}
    \includegraphics[width=0.32\textwidth]{figures/triton/tbar_Fit_Bin1.png}
    \includegraphics[width=0.32\textwidth]{figures/triton/tbar_Fit_Bin1.png}
    \caption{Bin by bin plots of the $^3\overline{\mathrm{H}}/{^3\mathrm{H}}$ ratio as a function of the varied inelastic cross section in Monte Carlo simulations, together with the one measured in data. The fitted line is an exponential fit according to the Lambert-Beer law \ref{eq:LambertBeer}, and is used to extract the cross section scaling factor.}
    \label{fig:Meth:RatiosAsFunctionsOfSigmaInel3H}
\end{figure}

In order to reconstruct the values of the inelastic cross section corresponding to the values of the scaling factor, 2 things are necessary: the average material of the ALICE detector (to pick the corresponding cross section implemented in Geant4) and the average energy loss of antinuclei before annihilation occurs (in order to multiply the correct momentum values). These two factors are discussed below in sections \ref{sec:Meth:AveragingMaterial} and \ref{sec:Meth:ELoss}, respectively.\\

For the TPC/TOF method, a similar method is used in order to extract the measurement of the inelastic cross section from the TPC/TOF ratio. However, due to the increased amount of material budget which particles have to traverse and the much reduced statistical uncertainties provided by the Pb--Pb data set, the exponential functions are much less steep in the area of interest. An example of such a fit is shown in figure \ref{fig:TPCTOF_fit}. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/TOF-TPC_ratio_xs_extraction.png}
    \caption{TPC/TOF ratio for \ahe\ as a function of the varied inelastic cross section, for one momentum bin. Figure taken from \cite{antiHe3XS}.}
    \label{fig:TPCTOF_fit}
\end{figure}


\subsubsection{Accounting for energy losses between the primary vertex and the point of annihilation}\label{sec:Meth:ELoss}
We collect the histograms leading to the antiparticle-to-particle ratio as functions of the momentum these particles have at the primary vertex $p_{Vtx}$. However, the cross section should be given as a function of the momentum which the particles have during annihilation, $p*$. This means energy losses which occur before annihilation need to be accounted for. Since we do not see tracks for particles which annihilate, this cannot be done on a case-by-case basis, but must be done statistically.\\

In order to correct for this, let us consider 2 extreme scenarios for where the annihilation might occur. If the annihilation occurs immediately when the particle is produced and meets the beampipe, then it will still have its initial momentum $p_{Vtx}$. The latest point at which a particle can annihilate and still be included, is just before it would get recognized by the detector. This can be denoted as $p_{\mathrm{TPC}}$ and $p_{\mathrm{TOF}}$, for the TPC and TOF regions of the analysis, respectively. $p_{\mathrm{TPC}}$ can be determined very accurately in data, since the tracking of the TPC allows the determination of the momentum. Since the annihilation must occur somewhere between these two momenta, we can assume a mean value of $p* = \frac{p_{\mathrm{TPC}} + p_{Vtx}}{2}$, and evaluate the uncertainties by evaluating the cross section using the 3 different scenarios MIN $p*=p_{Vtx}$, MEAN $p* = \frac{p_{\mathrm{TPC}} + p_{Vtx}}{2}$ and MAX $p* = p_{\mathrm{TPC}}$. A schematic representation of how this uncertainty is then applied to the inelastic cross section is shown in figure \ref{fig:Eloss_schematic}. The uncertainty from this correction is less than X\% for \ahe\ and Y\% for \atrit\ . $p_{\mathrm{TOF}}$ can similarly be found from the measurement of $\beta_{\mathrm{TOF}}$. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Eloss_corr_schematic.png}
    \caption{Schematic representation for how the correction for the energy loss of antinuclei -- and the corresponding systematic uncertainty -- is applied to the measurements of the inelastic cross sections. In order to map from the scaling factor to the inelastic cross section, the default parameterization used in Geant4 is employed.}
    \label{fig:Eloss_schematic}
\end{figure}





\subsubsection{Evaluating the average ALICE material budget}\label{sec:Meth:AveragingMaterial}
The annihilations in the ALICE detector can occur on any of the materials in the detector. Therefore, the inelastic cross section can only be shown on an average material. In order to obtain the average detector material, a weighted average is evaluated, based on the density of a given material $\rho$. This was evaluated over 1 cm steps (which are denoted as $i$) from 0 cm up to a radius $R$, which is the last position in the detector at which the particles could be identified. This means that annihilations before this point are accounted for in the inelastic cross section measurements. Since the distribution of the ALICE detector is non-uniform in azimuthal angle $\phi$, the material values were averaged over many random azimuthal angles (denoted as j). This is shown in equation \ref{eq:AveragingA}. 


\begin{equation}\label{eq:AveragingA}
<A> = 
\frac{\sum_{i=1}^R \rho_iA_i}{\sum_{i=1}^R \rho_i} = 
\frac{\sum_{i=1}^R \sum_{j=1}^N \rho_{ij}A_{ij}}{\sum_{i=1}^R \sum_{j=1}^N \rho_{ij}}
\end{equation}


The local $A$ and $Z$ values of the ALICE detector, as well as its density, as a function of radius is shown in figure \ref{fig:localMaterialValuesALICE}. This yields different values for different measuring methods, depending on the range of radii in which the annihilation can occur. If only the TPC is used for determining the antiparticle-to-particle ratio, then only the distance between the beampipe and the middle of the TPC (ca. 168 cm from the beampipe) is considered. When the TOF detector is also used, distances up to the TOF detector (370 cm) are considered. Finally, for the TOF/TPC method, the radii from the middle of the TPC (168 cm) to the TOF detector (370 cm) are evaluated. \\

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|}
              & TPC only & TPC + TOF & TPC/TOF matching \hline\\
        $<A>$ & 17.4 & 31.8 & 34.7 \hline\\
    \end{tabular}
    \caption{Values for the average atomic mass number of the ALICE detector material $<A>$, for different analysis methods. They are evaluated according to equation \ref{eq:AveragingA}.}
    \label{tab:MeanALabels}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/averageA.png}
    \caption{Local $A$ and $Z$ values (left) and density (right) of the ALICE detector material ad mid-rapidity as a function of the radial distance from the interaction point.}
    \label{fig:localMaterialValuesALICE}
\end{figure}

Once the measured scaling factor on the inelastic cross sections implemented in Geant4 are found, they need to multiplied by the corresponding inelastic cross sections. Since Geant4 only has cross sections implemented on existing materials, the one with the closest mass number $A$ was chosen, and then scaled according to the parameterizations used in Geant4, as described in equations \ref{eq:Glauber_singleA} and \ref{eq:Glauber_multA} in section \ref{sec:Intro:Glauber}.

\subsubsection{Uncertainty coming from the material budget}
The measurement outlined in this thesis relies on the accurate knowledge of the ALICE material budget. This is because the loss of antinuclei is proportional to $e^{-\sigma_\mathrm{inel} \rho l}$, where $\rho$ is the density of the material traversed and $l$ is the path length of the antinucleus through the material. Thus, the material budget can be quantified as the sum of $\rho_i l_i$, over all the materials $i$ in the detector. This means that the constraints on the cross section are actually on the product of the cross section and the material budget, and thus any uncertainty on the material budget is 1:1 applied to the inelastic cross section measurement. \\

Originally, the uncertainty on the material budget was quantified to be about 5\% using photon conversions\cite{photon_conversion}, up to the middle of the TPC (since later conversions would result in tracks which have less than half of the TPC clusters and thus cannot we well identified). However, this method left out the bulk of the material budget considered in any analysis using the TOF, as can be seen from figure \ref{fig:ALICE_detector_material_budget}. Therefore, the material between the TPC and TOF detectors needed to be validated using a different method. In order to to this, the same underlying idea as the TPC/TOF analysis was used, but rather than assuming a well known material budget and measuring the cross section, a particle with an accurately measured cross section was used in order to probe the material budget. The trick was to find such a particle which could be identified cleanly enough in the TPC alone. For this purpose, $\pi^+$ and $\pi^-$ from $\mathrm{K}_s^0 \rightarrow \pi^+ + \pi^-$ decays and protons from $\Lambda$ or $\overline{\Lambda}$ decays were used\cite{ALICE-PUBLIC-2022-002}. Due to their decay topology, they could be cleanly identified in the TPC alone, and their cross section was very accurately known. The measured ratio was then compared to ratios from simulation with varied ALICE material budget, in order to ascertain the uncertainty on the material budget between the TPC and TOF. The resulting uncertainties are shown in figure \ref{fig:ALICE_mat_budget}. It can be seen that an uncertainty of $\approx$ 5\% is achieved using this method. \\

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ALICE_material_budget_tests.png}
    \caption{Uncertainty on the ALICE material budget between the TPC and TOF detectors, as found by comparing the yields in the TPC to the ones in the TOF for pions from $\mathrm{K}_s^0$ decays (red) and protons from $\Lambda$ decays (black). The uncertainty is determined by comparing the measured ratio to ones obtained from detailed Monte Carlo simulations of the ALICE detector with varied material budgets, using Geant3 (left) and Geant4 (right). Figures taken from \cite{ALICE-PUBLIC-2022-001}.}
    \label{fig:ALICE_mat_budget}
\end{figure}

Therefore, a global uncertainty of about 5\% is assumed on the material budget. This uncertainty is included in the total uncertainty calculated on the the primordial ratio, which is the ratio of how many antiparticles are produced in respect to their particle equivalents, at the energies probed. This measurement is based on the $\bar{\mathrm{p}}/\mathrm{p}$ ratio as measured by ALICE \cite{primordial_ratio1, primordial_ratio2, Abbas_2013_primordial_ratio}. The uncertainty on this ratio is 1.5\%, which is applied for each nucleon in question for both analyses, i.e. 3\% for antideuterons and 4.5\% for \ahe\ and \atrit , which therefore includes the uncertainty on the material budget.
%\subsubsection{Evaluating the average material for antinuclei annihilations in the ALICE detector}

\subsubsection{Non-linear error propagation}
The experimental observable is the antiparticle-to-particle ratio, which is then used to calculate the inelastic cross section. Thus, it is necessary to propagate the errors from the former to the latter. This is however non-trivial, since the two are related via an exponential as described in the previous section. Thus, the initially symmetric uncertainties on the ratio become asymmetric when propagated to the inelastic cross section. Additionally, the systematic and statistical uncertainties -- which are independent on the ratio and thus sum up in quadrature -- can no longer be summed in quadrature on the inelastic cross section, since the scaling between them is no longer linear but exponential. Indeed, as the slope of the exponential is not known a priori, the uncertainties cannot be added at all without knowledge of the dependence of the antiparticle-to-particle ratio in a given bin on the inelastic cross section. This leaves two options for the representation of uncertainties on the inelastic cross section: i) show the statistical and systematic uncertainties separately, and give the parameterization of the exponential curve used to add them together for each bin or ii) sum the two uncertainties on the ratio and then propagate the total uncertainty. The second option is significantly more practical since it gives the reader immediate access to the total uncertainty, and does not require extra explanation. The separate uncertainties can be recovered using the fits shown in figures \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3He} and \ref{fig:Meth:RatiosAsFunctionsOfSigmaInel3H}. \\

An important note is that this asymmetry arises far more prominently in the antiparticle-to-particle analysis than in the TOF/TPC analysis. This is due to 2 factors: the much reduced statistical uncertainties for the TPC/TOF analysis and the increased material budget required (as opposed to the TPC only part of the antiparticle-to-particle analysis). This results in the fact that within the uncertainties, the effect of the inelastic cross section on the TPC/TOF ratio is well approximated with a linear function. Thus, the error propagation from the TPC/TOF ratio to the inelastic cross section has only barely noticeable asymmetries. 

%todo: do the anlytical error propagation when the two parameters are related by an exponential. 

\subsubsection{Systematic uncertainties}\label{sec:Meth:Systematics}
In this section we will discuss the sources of systematic uncertainties on the inelastic cross section measurements using the antiparticle-to-particle method for $A$=3 antinuclei. The uncertainties are dominated by statistical uncertainties at high momenta, while at low momenta the uncertainties are dominated by the correction for secondary nuclei. \\

The systematic uncertainties can be categorised into two camps: 1) accounting for explicit biases in the analysis techniques or 2) uncertainties coming from a lack of knowledge on one or more involved quantities. \\
An example of an explicit bias is the selection of bin sizes in the histograms used for particle identification. The size chosen was 0.5 $n\sigma_{\mathrm{TPC}}$, but in essence this choice could have just as well been 0.543211. In the limit of infinite statistics, the choice would not matter, but for the limited statistics present in this work, such choices can introduce a bias to the extracted (anti)nuclei yields, since the distributions are fitted in order to extract the yields. To account for this, values in which such an implicit bias occurred were varied around the chosen value until the extracted yields changed by $\pm$ 10\%, and the variance of the results were used to assign an uncertainty to this bias. Finally, before applying this uncertainty to the final results, a Barlow check was performed \cite{Barlow:2002yb}. A Barlow check is a statistical test which evaluates if the variance seen by changing a parameter is what is expected within statistical uncertainties, and ensures that an uncertainty is not doubly counted (once in the statistical uncertainty of the data, and once in the systematic uncertainty). There were two relevant uncertainties of this kind in the \ahe\ and \atrit\ analyses: track cuts and the PID procedure. The evaluation of the PID procedure is explained above, with the additional evaluation of the effect of the fit ranges, with the same method. The uncertainty due to the track cuts was slightly more complicated, due to the possible interdependence of different parameters. The track parameters on which cut were performed were each assigned a "tight", "default", and "loose" value, and the analysis was re-performed with random permutations of these cuts over 100 times, which is shown in figure \ref{fig:TrackingSystematics}. The standard deviation of these results was thus taken as the uncertainty. \\

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/he3_tracking_systematics.png}
    \includegraphics[width=0.48\textwidth]{figures/triton/Systematics_tritons.png}
    \caption{Evaluation of the systematic uncertainty due to track cuts, for \ahe\ (left) and \atrit\ (right). The same analysis was re-performed over 100 times with random permutations of "tight", "default", and "loose" cuts on each considered parameter.}
    \label{fig:TrackingSystematics}
\end{figure}

The second type of systematic uncertainty is due to lack of knowledge on a given parameter. An example of this is uncertainty on the material budget, which is only known to a precision of 4.5\% . This effect is evaluated on the primordial antiproton-to-proton ratio, causing an uncertainty of 1.5\% . Since the relationship between the  antiproton-to-proton ratio and the antiparticle-to-particle ratio for nuclei goes to the exponent of $A$ (see equation \ref{eq:CoalescenceParameter}), the resulting uncertainty on $A=3$ ratios is 4.5\% . The next uncertainty is due to the correction for secondary nuclei. This uncertainty is due to the limited statistics of both the templates and the data, making a fit between them difficult. The uncertainty can be seen from the uncertainties on the primary fraction, which are shown in figures \ref{} and \ref{}, for $^3\mathrm{He}$ and $^3\mathrm{H}$, respectively. This uncertainty is also applied on the antiparticle-to-particle ratios. Another uncertainty is the uncertainty on the measured matter inelastic cross section\footnote{Inelastic processes for matter can be hard scattering events which lead to breakup. The result is the same: a loss of the track in the analysed data.} on the antiparticle-to-particle ratio. This uncertainty was evaluated by varying the cross section in Monte Carlo simulations using Geant4, and was found to be 0.75\% below $p<1$ GeV/$c$, and 2.3 \% above this value. This is the same for \ahe\ and \atrit\, . The effect of the elastic cross section of both matter and antimatter were also studied, with an effect $< 1$\% for both \ahe\ and \atrit\ in all momentum bins. This is shown in figure \ref{fig:syst_uncertainties_elastic_xs}. \\

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/systematic_uncertainty_other_xs.png}
    \caption{Uncertainty on the antiparticle-to-particle ratio introduced by varying the elastic cross sections by 30\% (left) and the inelastic matter cross section by 10\% (right). Since the cross section for \atrit\ and \ahe\ are the same in Geant4, these values are valid for both species.}
    \label{fig:syst_uncertainties_elastic_xs}
\end{figure}

Finally, the uncertainty coming from the energy loss correction, which is due to our lack of knowledge at what momentum the annihilation occurs. It is applied during the extraction of the inelastic cross sections from the ratios, as described in section \ref{sec:Meth:ELoss}.   


\subsubsection{Bench-marking the method on the antiproton inelastic cross section}
In order to be sure that the antiparticle-to-particle methods gives an accurate measurement of the inelastic cross section, it first had to be benchmarked to a particle for which the inelastic cross section was well known. This was done for antiprotons in \cite{antideuteronXS}. The resulting inelastic cross sections are shown in figure \ref{fig:pbar_sigma_inel}. They match the parameterization implemented in Geant4 very well.\\

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/antiproton_inelastic_xs.png}
    \caption{The antiproton inelastic cross section on the average ALICE detector material, taken from \cite{antideuteronXS}.}
    \label{fig:pbar_sigma_inel}
\end{figure}

Since the antiproton inelastic cross section has been well measured, reproducing it with the antiparticle-to-particle method benchmarks the validity of the method. This allows its application to previously unmeasured quantities: the inelastic cross sections of $\overline{\mathrm{d}}$, $^3\overline{\mathrm{He}}$ and $^3\overline{\mathrm{H}}$. For \ahe\ and \atrit\ the TOF-to-TPC method was in addition to the antiparticle-to-particle method, to take advantage of the increased statistics available in Pb--Pb collisions. The validity of this new method had to be established by comparing it to the measurements using the already benchmarked antiparticle-to-particle method, showing not just the complementary nature of these two measurements, but also the necessity for both to be used in conjunction.
\subsubsection{Independence of collision system}
The antimatter-to-matter ratio method's dependence on collision system has been investigated by redoing the analysis performed in pPb collisions in \cite{antideuteronXS} for high multiplicity pp collisions. The dependence on the collision system is due to the differences in the collision energy, and the resulting difference in the primordial ratio is discussed in section \ref{sec:MCSim}. By taking the antiproton-to-proton ratio for the different collision systems and comparing them, the predicted difference between the antideuteron-to-deuteron ratio was obtained. The results are shown in figure \ref{fig:pp_pPb_dbardRatio}, which show that the differences between collisions systems are consistent with the expected deviation. Thus, the inelastic cross section measurements for the two are consistent. This independence of the inelastic cross section on the collision system is expected, since the inelastic cross section is completely independent on the collision system. This becomes especially self-evident when considering that the annihilations do not occur in the initial collisions, but rather as the antiparticles travel through the detector material.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/antiproton-to-proton-ratio-pp-pPb-comparison.png}
    \includegraphics[width=0.49\textwidth]{figures/antideuteron-to-deuteron-ratio-comparison-pp-pPb.png}
    \caption{Ratio of the antiproton-to-proton ratios (left) and antideuteron-to-deuteron ratios (right) obtained in high multiplicity pp collisions and in pPb collisions, compared to the expected difference from the different collision energies (dashed red line).}
    \label{fig:pp_pPb_dbardRatio}
\end{figure}

